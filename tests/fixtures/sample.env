# Sample environment configuration for do assistant
# DO_MODEL (string): HF repo[:file] identifier for llama.cpp.
DO_MODEL=example/repo:demo.gguf
# DO_MODEL_BRANCH (string): branch or tag to fetch from.
DO_MODEL_BRANCH=main
# DO_MODEL_CACHE (string): cache directory where models reside.
DO_MODEL_CACHE=./models
# DO_SUPERVISED (bool): true to require confirmation before running tools.
DO_SUPERVISED=false
# DO_VERBOSITY (int): 0=quiet, 1=info, 2=debug.
DO_VERBOSITY=2
# LLAMA_BIN (string): llama.cpp binary to use for scoring; can be a stub during testing.
LLAMA_BIN=./bin/llama-mock
