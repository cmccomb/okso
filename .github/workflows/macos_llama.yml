name: macOS llama.cpp runtime

on:
  push:
    branches: [main]
  pull_request:

env:
  MODEL_SPEC: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
  MODEL_BRANCH: main
  LLAMA_BIN: llama-cli
  APPROVE_ALL: true
  USE_REACT_LLAMA: true
  HF_HOME: /Users/runner/Library/Caches/huggingface

jobs:
  macos-runtime:
    runs-on: macos-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Homebrew dependencies
        shell: bash
        run: |
          set -euxo pipefail
          brew update
          brew install cmake pkg-config python jq coreutils git-lfs bats-core

      - name: Build llama.cpp (llama-cli)
        shell: bash
        run: |
          set -euxo pipefail
          exec > >(tee llama-build.log) 2>&1
          git clone --depth 1 https://github.com/ggerganov/llama.cpp.git
          cmake -S llama.cpp -B llama.cpp/build -DLLAMA_BUILD_SERVER=OFF -DLLAMA_BUILD_EXAMPLES=OFF -DLLAMA_BUILD_TESTS=OFF
          cmake --build llama.cpp/build --target llama-cli --config Release
          echo "${PWD}/llama.cpp/build/bin" >> "${GITHUB_PATH}"

      - name: Install Python dependencies
        shell: bash
        run: |
          set -euxo pipefail
          python3 -m pip install --upgrade pip
          python3 -m pip install huggingface_hub

      - name: Prime Hugging Face cache with tiny model
        shell: bash
        env:
          MODEL_SPEC: ${{ env.MODEL_SPEC }}
          MODEL_BRANCH: ${{ env.MODEL_BRANCH }}
        run: |
          set -euxo pipefail
          exec > >(tee model-download.log) 2>&1
          mkdir -p "${HF_HOME}"
          python3 - <<'PY'
import os
from huggingface_hub import snapshot_download

model_spec = os.environ.get("MODEL_SPEC", "")
model_branch = os.environ.get("MODEL_BRANCH", "main")

if ":" not in model_spec:
    raise SystemExit("MODEL_SPEC must be in repo:filename format")

repo_id, filename = model_spec.split(":", 1)
snapshot_download(
    repo_id=repo_id,
    revision=model_branch,
    allow_patterns=[filename],
    local_dir=None,
    local_dir_use_symlinks=False,
)
PY

      - name: Run macOS llama runtime test
        shell: bash
        env:
          MODEL_SPEC: ${{ env.MODEL_SPEC }}
          MODEL_BRANCH: ${{ env.MODEL_BRANCH }}
        run: |
          set -euxo pipefail
          bats --print-output-on-failure tests/runtime/test_macos_tiny_llama.bats | tee macos-e2e.log

      - name: Upload macOS artifacts on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: macos-llama-debug
          path: |
            llama-build.log
            model-download.log
            macos-e2e.log
            ${{ env.HF_HOME }}
